npudensbw(x)
fit <- npudensbw(x)
fit
names(fit)
fit$bw
fitbw <- npudens(fit)
fitbw
grbg <- predict(fitbw, seq(-4,4,length = 1000))
grbg <- predict(fitbw, newdata = seq(-4,4,length = 1000))
grbg
xseq <- seq(-4,4,length = 1000)
plot(grbg~xseq)
length(grbg)
?predict.npudens
?np:::predict.npudens
?np::predict.npudens
class(fitbw)
?np::predict.npdensity
?np:::predict.npdensity
grbg <- predict(fitbw, newdata = seq(-4,4,length = 1000))
grbg
length(grbg)
class(fitbw)
predict.npdensity
np:::predict.npdensity
?npudens
grbg <- predict(fitbw, edat = seq(-4,4,length = 1000))
grbg
plot(grbg~xseq)
names(fit)
fit$fval
?pctile
?percentile
?pctile
?quantile
n <- 100#
p <- 30#
X <- matrix(rnorm(n*p), nrow = n, ncol = p)#
Y <- rbinom(n, 1, plogis(10*X[,1] + 20*X[,10]))
#' Wrapper for fitting a main terms random forest#
#' #
#' @param train ...#
#' @param test ...#
#' @return A list#
#' @export#
#' @importFrom randomForest randomForest #
#' @importFromt stats predict#
#' @examples#
#' # TO DO: Add#
randomforest_wrapper <- function(train, test,#
                                 mtry = floor(sqrt(ncol(train$X))), #
    ntree = 1000, nodesize = 1, maxnodes = NULL, importance = FALSE,...){#
    rf_fit <- randomForest::randomForest(y = as.factor(train$Y), #
            x = train$X, ntree = ntree, xtest = rbind(test$X, train$X), #
            keep.forest = TRUE, mtry = mtry, nodesize = nodesize, #
            maxnodes = maxnodes, importance = importance, ...)#
    all_psi <- rf_fit$test$votes[,2]#
    ntest <- length(test$Y)#
    ntrain <- length(train$Y)#
    psi_nBn_testx <- all_psi[1:ntest]#
    psi_nBn_trainx <- all_psi[(ntest+1):(ntest+ntrain)]#
    return(list(psi_nBn_trainx = psi_nBn_trainx, psi_nBn_testx = psi_nBn_testx,#
                model = rf_fit, train_y = train$Y, test_y = test$Y))#
}#
#
#' Wrapper for fitting a main terms GLM#
#' #
#' @param train ...#
#' @param test ...#
#' @return A list#
#' @export#
#' @importFrom stats glm predict#
#' @examples#
#' # TO DO: Add#
glm_wrapper <- function(train, test){#
    glm_fit <- stats::glm(train$Y ~ ., data = train$X, family = binomial())#
    Psi_nBn_0 <- function(x){#
      stats::predict(glm_fit, newdata = x, type = "response")#
    }#
    psi_nBn_trainx <- Psi_nBn_0(train$X)#
    psi_nBn_testx <- Psi_nBn_0(test$X)#
    return(list(psi_nBn_trainx = psi_nBn_trainx, psi_nBn_testx = psi_nBn_testx,#
                model = glm_fit, train_y = train$Y, test_y = test$Y))#
}#
#
#' Wrapper for fitting lasso #
#' @param train ...#
#' @param test ...#
#' @return A list#
#' @export#
#' @importFrom glmnet cv.glmnet#
#' @examples#
#' # TO DO: Add#
glmnet_wrapper <- function(train, test){#
    glmnet_fit <- glmnet::cv.glmnet(x = train$X, y = train$Y,#
        lambda = NULL, type.measure = "deviance", nfolds = 5, #
        family = "binomial", alpha = 0.5, nlambda = 100)#
    Psi_nBn_0 <- function(x){#
      stats::predict(glmnet_fit, newx = x, type = "response")#
    }#
    psi_nBn_trainx <- Psi_nBn_0(train$X)#
    psi_nBn_testx <- Psi_nBn_0(test$X)#
    return(list(psi_nBn_trainx = psi_nBn_trainx, psi_nBn_testx = psi_nBn_testx,#
                model = glmnet_fit, train_y = train$Y, test_y = test$Y))#
}
n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)
#' Compute CVTML estimates of cross-validated AUC#
#' #
#' TO DO: Add#
#' @param Y The outcome#
#' @param X The predictors#
#' @param K The number of folds#
#' @param learner The learner wrapper#
#' @param seed A random seed to set#
#' @param parallel Compute the predictors in parallel?#
#' @param maxIter Maximum number of iterations for cvtmle#
#' @param icTol Iterate until maxIter is reach or mean of cross-validated#
#' efficient influence function is less than \code{icTol}#
#' @param ... other arguments, not currently used#
#' @importFrom SuperLearner CVFolds#
#' @importFrom cvAUC ci.cvAUC#
#' @importFrom stats uniroot#
#' @export#
#' @return A list#
#' @examples#
#' n <- 200#
#' p <- 10#
#' X <- matrix(rnorm(n*p), nrow = n, ncol = p)#
#' Y <- rbinom(n, 1, plogis(X[,1] + X[,10]))#
#' fit <- cvauc_cvtmle(Y = Y, X = X, K = 5, learner = "glm_wrapper")#
cvauc_cvtmle <- function(Y, X, K, learner = "glm_wrapper", #
                         seed = 1234,#
                         parallel = FALSE, maxIter = 10, #
                         icTol = 1/length(Y), #
                         ...){#
  n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)#
#
  # initial distributions of psi in training samples#
  # dist_psix_y0 <- lapply(prediction_list, .getPsiDistribution, y = 0)#
  # dist_psix_y1 <- lapply(prediction_list, .getPsiDistribution, y = 1)#
  # make long data for targeting step#
  long_data_list <- lapply(prediction_list, .makeLongData, gn = mean(Y))#
  # full_long_data <- Reduce(rbind, long_data_list)#
  # full_long_data$outcome <- with(full_long_data, as.numeric(psi <= u))#
  # full_long_data$logit_Fn <- SuperLearner::trimLogit(full_long_data$Fn, .Machine$double.neg.eps)#
  # targeting#
  epsilon_0 <- rep(0, maxIter)#
  epsilon_1 <- rep(0, maxIter)#
  iter <- 0#
  update_long_data_list <- long_data_list#
  # combine list into data frame#
  full_long_data <- Reduce(rbind, update_long_data_list)#
  # compute mean of EIF#
  D1 <- .Dy(full_long_data, y = 1)#
  D0 <- .Dy(full_long_data, y = 0)#
  ic <- D1 + D0 #
  PnDstar <- mean(ic)#
#
  # compute initial estimate of cvAUC#
  # compute estimated cv-AUC #
  dist_psix_y0_star <- lapply(prediction_list, .getPsiDistribution, #
                         y = 0, epsilon = epsilon_0)#
  dist_psix_y1_star <- lapply(prediction_list, .getPsiDistribution, y = 1,#
                         epsilon = epsilon_1)#
#
  # get AUC#
  init_auc <- mean(mapply(FUN = .getAUC, dist_y0 = dist_psix_y0_star, #
                     dist_y1 = dist_psix_y1_star))#
  est_onestep <- init_auc + PnDstar#
  se_onestep <- sqrt(var(ic)/n)#
#
  # estimating equations#
  est_esteq <- stats::uniroot(.estimatingFn, interval = c(0, 1), #
                   prediction_list = prediction_list, gn = mean(Y))$root#
  se_esteq <- se_onestep#
#
  tmle_auc <- rep(NA, maxIter)#
  PnDstar <- Inf#
  while(PnDstar > icTol & iter < maxIter){#
    iter <- iter + 1#
    # targeting with different epsilon#
    # if(TRUE){#
    ###############
    # target F0#
    ###############
    # make weight for loss function#
    full_long_data$targeting_weight_0 <- #
      as.numeric(full_long_data$Y == 0)/(2*full_long_data$gn) * full_long_data$dFn #
    # fit intercept only model with weights#
    suppressWarnings(#
      fluc_mod_0 <- glm(outcome ~ offset(logit_Fn), family = binomial(),#
                        data = full_long_data[full_long_data$Yi == 0,], #
                        weights = full_long_data$targeting_weight_0[full_long_data$Yi == 0],#
                        start = 0)#
    )#
    epsilon_0[iter] <- as.numeric(fluc_mod_0$coef[1])#
    # if unstable glm fit refit using optim#
    coef_tol <- 1e2#
    if(abs(fluc_mod_0$coef) > coef_tol){#
      fluc_mod_0 <- optim(fluc_mod_optim_0, method = "Brent", par = 0, #
              fld = full_long_data[full_long_data$Yi == 0,],#
              lower = -coef_tol, upper = coef_tol, #
              control = list(reltol = 1e-14))#
      epsilon_0[iter] <- as.numeric(fluc_mod_0$par)#
    }#
    # update values in long_data_list#
    update_long_data_list <- lapply(prediction_list, .makeLongData, gn = mean(Y),#
                             epsilon_0 = epsilon_0, epsilon_1 = epsilon_1,#
                             update = TRUE)#
    # update full long data#
    full_long_data <- Reduce(rbind, update_long_data_list)#
    # sanity check#
    D0_tmp <- c(.Dy(full_long_data, y = 0))#
#
    # make weight for loss function#
    full_long_data$targeting_weight_1 <- #
      as.numeric(full_long_data$Y == 1)/(2*full_long_data$gn) * full_long_data$dFn #
    # fit intercept only model with weights#
    suppressWarnings(#
      fluc_mod_1 <- glm(outcome ~ offset(logit_Fn), family = binomial(),#
                        data = full_long_data[full_long_data$Yi == 1,], #
                        weights = full_long_data$targeting_weight_1[full_long_data$Yi == 1],#
                        start = 0)#
    )#
    # update values in long_data_list#
    epsilon_1[iter] <- as.numeric(fluc_mod_1$coef[1])#
    if(abs(fluc_mod_1$coef) > coef_tol){#
      fluc_mod_1 <- optim(fluc_mod_optim_1, method = "Brent", par = 0, #
              fld = full_long_data[full_long_data$Yi == 1,],#
              lower = -coef_tol, upper = coef_tol, #
              control = list(reltol = 1e-14))#
      epsilon_1[iter] <- as.numeric(fluc_mod_1$par)#
    }#
    update_long_data_list <- lapply(prediction_list, .makeLongData, gn = mean(Y),#
                             epsilon_0 = epsilon_0, epsilon_1 = epsilon_1,#
                             update = TRUE)#
    # update full long data#
    full_long_data <- Reduce(rbind, update_long_data_list)#
    # compute mean of EIF#
    D1 <- .Dy(full_long_data, y = 1)#
    D0 <- .Dy(full_long_data, y = 0)#
    ic <- D1 + D0 #
    PnDstar <- mean(ic)#
#
    # compute estimated cv-AUC #
    dist_psix_y0_star <- lapply(prediction_list, .getPsiDistribution, #
                           y = 0, epsilon = epsilon_0)#
    dist_psix_y1_star <- lapply(prediction_list, .getPsiDistribution, y = 1,#
                           epsilon = epsilon_1)#
#
    # get AUC#
    tmle_auc[iter] <- mean(mapply(FUN = .getAUC, dist_y0 = dist_psix_y0_star, #
                       dist_y1 = dist_psix_y1_star))#
#
  }#
#
    # fluc_mod_0 <- optim(par = 0, fn = .lossF0, long_data_list = long_data_list,#
    #                   method = "Brent", lower = -0.05, upper = 0.05,#
    #                   control = list(reltol = 1e-9))#
    # ic_0 <- .D0(long_data_list, epsilon = fluc_mod_0$par)#
    # fluc_mod_1 <- optim(par = 0, fn = .lossF1, long_data_list = long_data_list,#
    #                   method = "Brent", lower = -0.05, upper = 0.05,#
    #                   control = list(reltol = 1e-9))#
    # ic_1 <- .D1(long_data_list, epsilon = fluc_mod_1$par)#
    # compute regular cvAUC#
    valid_pred_list <- lapply(prediction_list, "[[", "psi_nBn_testx")#
    valid_label_list <- lapply(prediction_list, "[[", "test_y")#
    regular_cvauc <- cvAUC::ci.cvAUC(predictions = valid_pred_list,#
                                labels = valid_label_list)#
    est_empirical <- regular_cvauc$cvAUC#
    se_empirical <- regular_cvauc$se#
    # # true CV AUC#
    # N <- 1e5#
    # p <- 100#
    # bigX <- matrix(rnorm(N*p), nrow = N, ncol = p)#
    # bigY <- rbinom(N, 1, plogis(bigX[,1] + bigX[,10] + bigX[,20]))#
    # big_valid_pred_list <- lapply(prediction_list, function(x){#
    #   predict(x$model, newx = bigX, type = "response")#
    # })#
    # big_label_list <- list(bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY,#
    #                        bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY)#
    # true_cvauc <- mean(cvAUC::AUC(predictions = big_valid_pred_list,#
    #                         labels = big_label_list))#
#
    # format output#
    out <- list()#
    out$est_cvtmle <- tmle_auc[iter]#
    out$iter_cvtmle <- iter#
    out$cvtmle_trace <- tmle_auc#
    out$se_cvtmle <- sqrt(var(ic)/n)#
    out$est_init <- init_auc#
    out$est_empirical <- est_empirical#
    out$se_empirical <- se_empirical#
    out$est_onestep <- est_onestep#
    out$se_onestep <- se_onestep#
    out$est_esteq <- est_esteq#
    out$se_esteq <- se_esteq#
#
    out$models <- lapply(prediction_list, "[[", "model")#
    class(out) <- "cvauc"#
    return(out)#
}#
#
#' Alternative fluctuation routine #
#' @param epsilon Fluctuation parameter #
#' @param fld full_long_data_list#
fluc_mod_optim_0 <- function(epsilon, fld, tol = 1e-3){#
  p_eps <- plogis(fld$logit_Fn + epsilon)#
  p_eps[p_eps == 1] <- 1 - tol#
  p_eps[p_eps == 0] <- tol#
  loglik <- -sum(fld$targeting_weight_0 * (fld$outcome * log(p_eps) + (1-fld$outcome) * log(1 - p_eps)))#
  return(loglik)#
}#
#' Alternative fluctuation routine #
#' @param epsilon Fluctuation parameter #
#' @param fld full_long_data_list#
fluc_mod_optim_1 <- function(epsilon, fld, tol = 1e-3){#
  p_eps <- plogis(fld$logit_Fn + epsilon)#
  p_eps[p_eps == 1] <- 1 - tol#
  p_eps[p_eps == 0] <- tol#
  loglik <- -sum(fld$targeting_weight_1 * (fld$outcome * log(p_eps) + (1-fld$outcome) * log(1 - p_eps)))#
  return(loglik)#
}#
#
#' An estimating function for cvAUC#
#' @param auc The value of auc to find root for#
#' @param prediction_list Entry in prediction_list#
#' @param gn Marginal probability of outcome#
.estimatingFn <- function(auc = 0.5, prediction_list, gn){#
  # get first influence function piece for everyone#
  ic_1 <- #
  Reduce("c",lapply(prediction_list, function(x){#
    thisFn <- sapply(1:length(x$test_y), function(i){#
      ifelse(x$test_y[i] == 1, #
             F_nBn_star(x$psi_nBn_testx[i], y = 0, Psi_nBn_0 = x$psi_nBn_trainx,#
                        Y_Bn = x$train_y)/ gn , #
             (1 - F_nBn_star(x$psi_nBn_testx[i], y = 1, Psi_nBn_0 = x$psi_nBn_trainx,#
                        Y_Bn = x$train_y))/(1 - gn))#
    })#
  }))#
  all_y <- unlist(lapply(prediction_list, "[[", "test_y"))#
  ic_2 <- rep(0, length(all_y))#
  ic_2[all_y == 0] <- - auc / (1 - gn)#
  ic_2[all_y == 1] <- - auc / gn#
  return(mean(ic_1 + ic_2))#
}#
#
# uniroot(.estimatingFn, interval = c(0, 1), prediction_list = prediction_list, gn = mean(Y))#
#
#' Compute a portion of the efficient influence function#
#' @param full_long_data A long form data set#
#' @param y Which portion of the EIF to compute#
#' @return Vector of EIF#
.Dy <- function(full_long_data, y){#
  by(full_long_data, full_long_data$id, function(x){#
    sum((-1)^y * as.numeric(x$Y == y)/(x$gn) * (x$outcome - x$Fn) * x$dFn)#
  })#
}#
#
#' Compute the (CV)TMLE cumulative dist at psi_x#
#' @param psi_x Value to compute conditional (on Y=y) cdf of Psi#
#' @param y Value of Y to condition on #
#' @param Psi_nBn_0 Values of Psi_nBn(X) from training sample#
#' @param Y_Bn Values of Y from training sample#
#' @param epsilon Vector of fluctuation parameter estimates#
#' @return Numeric value of CDF#
F_nBn_star <- function(psi_x, y, Psi_nBn_0, Y_Bn, epsilon = 0, #
                       # tol = .Machine$double.neg.eps#
                       tol = 1e-3#
                       ){#
  plogis(SuperLearner::trimLogit(mean(Psi_nBn_0[Y_Bn == y] <= psi_x), tol) +#
          sum(epsilon))#
}#
#
#' Worker function to make long form data set needed for#
#' CVTMLE targeting step #
#' #
#' @param x An entry in the "predictions list" that has certain#
#' named values (see \code{?.getPredictions})#
#' @param gn An estimate of the marginal dist. of Y#
#' @param update Boolean of whether this is called for initial#
#' construction of the long data set or as part of the targeting loop. #
#' If the former, empirical "density" estimates are used. If the latter#
#' these are derived from the targeted cdf. #
#' @param epsilon_0 If \code{update = TRUE}, a vector of TMLE fluctuation#
#' parameter estimates used to add the CDF and PDF of Psi(X) to the data set#
#' @param epsilon_1 Ditto above#
#' #
#' @return A long form data list of a particular set up. Columns are named id #
#' (multiple per obs. in validation sample), u (if Yi = 0, these are the values of psi(x) in the#
#' training sample for obs with Y = 1, if Yi = 1, these are values of psi(x) in#
#' the training sample for obs. with Y = 0), Yi (this id's value of Y), Fn (#
#' estimated value of the cdf of psi(X) given Y = Yi in the training sample), #
#' dFn (estimated value of the density of psi(X) given Y = (1-Yi) in the #
#' training sample), psi (the value of this observations \hat{\Psi}(P_{n,B_n}^0)),#
#' gn (estimate of marginal of Y e.g., computed in whole sample), outcome (indicator#
#' that psix <= u), logit_Fn (the cdf estimate on the logit scale, needed for #
#' offset in targeting model).#
#
.makeLongData <- function(x, gn, update = FALSE, epsilon_0 = 0, epsilon_1 = 0,#
                          # tol = .Machine$double.neg.eps, #
                          tol = 1e-3#
                          ){#
  # first the dumb way, writing a loop over x$psi_nBn_testx#
  uniq_train_psi_y0 <- sort(unique(x$psi_nBn_trainx[x$train_y == 0]))#
  uniq_train_psi_y1 <- sort(unique(x$psi_nBn_trainx[x$train_y == 1]))#
  # ord_train_psi_y0 <- order(x$psi_nBn_trainx[x$train_y == 0])#
  # ord_train_psi_y1 <- order(x$psi_nBn_trainx[x$train_y == 1])#
  n_valid <- length(x$psi_nBn_testx)#
  n_train <- length(x$psi_nBn_trainx)#
  n1_train <- sum(x$train_y)#
  n1_uniq_train <- length(uniq_train_psi_y1)#
  n0_train <- n_train - n1_train#
  n0_uniq_train <- length(uniq_train_psi_y0)#
  n1_valid <- sum(x$test_y)#
  n0_valid <- n_valid - n1_valid#
  valid_ids <- as.numeric(names(x$psi_nBn_testx))#
  tot_length <- n1_valid * n0_uniq_train + n0_valid * n1_uniq_train#
#
  idVec <- rep(NA, tot_length)#
  uVec <- rep(NA, tot_length)#
  YuVec <- rep(NA, tot_length)#
  YiVec <- rep(NA, tot_length)#
  F_1n_Bn_uVec <- rep(NA, tot_length)#
  F_0n_Bn_uVec <- rep(NA, tot_length)#
  dF_1n_Bn_uVec <- rep(NA, tot_length)#
  dF_0n_Bn_uVec <- rep(NA, tot_length)#
  FnVec <- rep(NA, tot_length)#
  dFnVec <- rep(NA, tot_length)#
  psiVec <- rep(NA, tot_length)#
#
  # cumulative dist of psi_n | Y = 1 evaluated at all values of #
  # psi_n associated with Y = 0 observations#
  F1nBn <- sapply(uniq_train_psi_y0, #
                  F_nBn_star, Psi_nBn_0 = x$psi_nBn_trainx, y = 1, #
                  Y_Bn = x$train_y, epsilon = epsilon_1)#
  # cumulative dist of psi_n | Y = 0 evaluated at all values of #
  # psi_n associated with Y = 1 observations#
  F0nBn <- sapply(uniq_train_psi_y1, #
                  F_nBn_star, Psi_nBn_0 = x$psi_nBn_trainx, y = 0, #
                  Y_Bn = x$train_y, epsilon = epsilon_0)#
#
  # empirical dens of psi_n | Y = 1 and Y = 0 evaluated at all #
  if(!update){#
    dF1nBn <- as.numeric(table(x$psi_nBn_trainx[x$train_y == 1])/n1_train)#
    dF0nBn <- as.numeric(table(x$psi_nBn_trainx[x$train_y == 0])/n0_train)#
  }else{#
    # cumulative dist of psi_n | Y = 1 evaluated at all values of #
    # psi_n associated with Y = 0 observations#
    F1nBn_1 <- sapply(uniq_train_psi_y1, #
                    F_nBn_star, Psi_nBn_0 = x$psi_nBn_trainx, y = 1, #
                    Y_Bn = x$train_y, epsilon = epsilon_1)#
    dF1nBn <- diff(c(0, F1nBn_1))#
    # cumulative dist of psi_n | Y = 0 evaluated at all values of #
    # psi_n associated with Y = 1 observations#
    F0nBn_0 <- sapply(uniq_train_psi_y0, #
                    F_nBn_star, Psi_nBn_0 = x$psi_nBn_trainx, y = 0, #
                    Y_Bn = x$train_y, epsilon = epsilon_0)#
    dF0nBn <- diff(c(0, F0nBn_0))#
  }#
#
  # loop over folks in validation fold#
  cur_start <- 1#
  for(i in seq_len(n_valid)){#
    if(x$test_y[i] == 0){#
      cur_end <- cur_start + n1_uniq_train - 1#
      idVec[cur_start:cur_end] <- x$valid_ids[i]#
      # ordered unique values of psi in training | y = 1#
      uVec[cur_start:cur_end] <- uniq_train_psi_y1#
      # value of this Y_i#
      YiVec[cur_start:cur_end] <- 0#
      # cdf of psi | y = 0 in training at each u#
      FnVec[cur_start:cur_end] <- F0nBn#
      # pdf of psi | y = 1 in training at each u#
      dFnVec[cur_start:cur_end] <- dF1nBn#
      # vector of this psi#
      psiVec[cur_start:cur_end] <- x$psi_nBn_testx[i]#
    }else{#
      cur_end <- cur_start + n0_uniq_train - 1#
      idVec[cur_start:cur_end] <- x$valid_ids[i]#
      # ordered unique values of psi in training | y = 0#
      uVec[cur_start:cur_end] <- uniq_train_psi_y0#
      # value of this Y_i#
      YiVec[cur_start:cur_end] <- 1#
      # cdf of psi | y = 1 in training at each u#
      FnVec[cur_start:cur_end] <- F1nBn#
      # pdf of psi | y = 0 in training at each u#
      dFnVec[cur_start:cur_end] <- dF0nBn#
      # vector of this psi#
      psiVec[cur_start:cur_end] <- x$psi_nBn_testx[i]#
    }#
    cur_start <- cur_end + 1#
  }#
#
  out <- data.frame(id = idVec, u = uVec,#
                   Yi = YiVec, Fn = FnVec, dFn = dFnVec,#
                   psi = psiVec)#
#
  # add in gn#
  out$gn <- NA#
  out$gn[out$Yi == 1] <- gn#
  out$gn[out$Yi == 0] <- 1 - gn#
  # add in "outcome"#
  out$outcome <- as.numeric(out$psi <= out$u)#
  # add in logit(Fn)#
  out$logit_Fn <- SuperLearner::trimLogit(out$Fn, tol)#
  return(out)#
}#
#' Compute the AUC given the cdf and pdf of psi #
#' #
#' See \code{?.getPsiDistribution} to understand expected input format#
#' #
#' @param dist_y0 Distribution of psi given Y = 0#
#' @param dist_y1 Distribution of psi given Y = 1#
#' @return Numeric #
# TO DO: make more efficient#
# TO DO: how are ties handled in findInterval?#
.getAUC <- function(dist_y0, dist_y1){#
  tot <- 0#
  for(i in seq_along(dist_y0$psix)){#
    idx <- findInterval(x = dist_y0$psix[i], vec = dist_y1$psix)#
    p1 <- ifelse(idx == 0, 1, (1 - dist_y1$Fn[idx]))#
    p2 <- dist_y0$dFn[i]#
    tot <- tot + p1 * p2#
  }#
  return(tot)#
}#
#
#' Compute the conditional (given Y = y) estimated distribution of psi#
#' #
#' @param x An entry in the output from .getPredictions#
#' @param y What value of Y to compute dist. est. #
#' @param epsilon A vector of estimated coefficients form tmle fluctuation #
#' submodels. #
#' #
#' @return A data.frame with the distribution of psi given Y = y with names#
#' psix (what value estimates are evaluated at), dFn (density estimates),#
#' Fn (cdf estimates)#
.getPsiDistribution <- function(x, y, epsilon = 0){#
    this_n <- length(x$psi_nBn_trainx[x$train_y == y])#
    uniq_train_psi_y <- sort(unique(x$psi_nBn_trainx[x$train_y == y]))#
    FynBn_y <- sapply(uniq_train_psi_y, #
                F_nBn_star, Psi_nBn_0 = x$psi_nBn_trainx, y = y, #
                Y_Bn = x$train_y, epsilon = epsilon)#
    dFynBn <- diff(c(0, FynBn_y))#
    out <- data.frame(psix = uniq_train_psi_y, #
                      dFn = dFynBn,#
                      Fn = FynBn_y)#
    return(out)#
}#
#' Worker function for fitting prediction functions (possibly in parallel)#
#' #
#' @param learner The wrapper to use#
#' @param Y The outcome#
#' @param X The predictors#
#' @param K The number of folds#
#' @param parallel Whether to compute things in parallel using future#
#' #
#' @return A list of the result of the wrapper executed in each fold#
.getPredictions <- function(learner, Y, X, K, folds, parallel){#
#
  .doFit <- function(x, tmpX, Y, folds, learner){#
    out <- do.call(learner, args=list(train = list(Y = Y[-folds[[x]]], X = tmpX[-folds[[x]],,drop=FALSE]),#
                                      test = list(Y = Y[folds[[x]]], X = tmpX[folds[[x]],,drop=FALSE])))#
    out$valid_ids <- folds[[x]]#
    return(out)#
  }#
#
  if(parallel){#
    stop("Parallel processing code needs to be re-written.")#
    # cl <- makeCluster(detectCores())#
    # registerDoParallel(cl)#
    # predFitList <- foreach(v = 1:length(folds), .export=learner) %dopar% #
    #   .doFit(v, tmpX = X, Y = Y, folds = folds, learner = learner)#
    # stopCluster(cl)#
  }else{#
    predFitList <- lapply(split(seq(K),factor(seq(K))),FUN = .doFit, tmpX = X, Y = Y, folds = folds, learner = learner)#
  }#
  # return results#
  return(predFitList)#
}
n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)
seed <- 1234
n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)
K <- 5
n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)
learner <- "glm_wrapper"
n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)
X <- data.frame(matrix(rnorm(n*p), nrow = n, ncol = p))
Y <- rbinom(n, 1, plogis(10*X[,1] + 20*X[,10]))
K <- 5
n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)
table(Y)
p <- 5
X <- data.frame(matrix(rnorm(n*p), nrow = n, ncol = p))
Y <- rbinom(n, 1, plogis(X[,1] + X[,10]))
K <- 5
learner <- "glm_wrapper"
Y <- rbinom(n, 1, plogis(X[,1] + X[,2]))
n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)#
  # get quantile estimates#
  quantile_list <- .getQuantile(prediction_list, p = sens)
.getQuantile <- function(x,p){#
  quantile(x$psi_nBn_trainx, p = p)#
}
n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)#
  # get quantile estimates#
  quantile_list <- .getQuantile(prediction_list, p = sens)
sens <- 0.95
.getQuantile <- function(x,p){#
  quantile(x$psi_nBn_trainx, p = p)#
}
n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)#
  # get quantile estimates#
  quantile_list <- .getQuantile(prediction_list, p = sens)
quantile_list
quantile_list <- lapply(prediction_list, .getQuantile, p = sens)
quantile_list
#' @importFrom np #
.getDensity <- function(x, c0, ... ){#
  # density given y = 1#
  fitbw <- npudensbw(x$psi_nBn_trainx[x$train_y == 1])#
  fit <- npudens(fitbw)#
  # estimate at c0#
  f_10_c0 <- predict(fit, edata = c0)#
#
  # marginal density#
  fitbw_marg <- npudensbw(x$psi_nBn_trainx)#
  fit_marg <- npudens(fitbw_marg)#
  # estimate at c0#
  f_0_c0 <- predict(fit_marg, edata = c0)#
#
  # return both#
  return(list(f_10_c0 = f_10_c0, f_0_c0 = f_0_c0))#
}
debug(.getDensity)
density_list <- mapply(x = prediction_list, c0 = quantile_list,
FUN = .getDensity, SIMPLIFY = FALSE)
pseq <- seq(0,1,length = 500)
grbg <- predict(fit, edata = pseq)
head(grbg)
plot(grbg ~ pseq)
length(grbg)
length(pseq)
fit
f_10_c0
Q
.getDensity <- function(x, c0, ... ){#
  # density given y = 1#
  fitbw <- npudensbw(x$psi_nBn_trainx[x$train_y == 1])#
  fit <- npudens(fitbw)#
  # estimate at c0#
  f_10_c0 <- predict(fit, edat = c0)#
#
  # marginal density#
  fitbw_marg <- npudensbw(x$psi_nBn_trainx)#
  fit_marg <- npudens(fitbw_marg)#
  # estimate at c0#
  f_0_c0 <- predict(fit_marg, edat = c0)#
#
  # return both#
  return(list(f_10_c0 = f_10_c0, f_0_c0 = f_0_c0))#
}
debug(.getDensity)
density_list <- mapply(x = prediction_list, c0 = quantile_list,
FUN = .getDensity, SIMPLIFY = FALSE)
fit
f_10_c0
grbg <- predict(fit, edat = pseq)
grbg <- predict(fit, edat = p_seq)
pseq <- seq(0,1,length = 500)
grbg <- predict(fit, edat = p_seq)
grbg <- predict(fit, edat = pseq)
plot(grbg ~ pseq)
hist(x$psi_nBn_trainx)
hist(x$psi_nBn_trainx, freq = FALSE)
lines(grbg ~ pseq, col = 2)
grbg <- predict(fit_marg, edat = pseq)
plot(grbg ~ pseq)
hist(x$psi_nBn_trainx)
lines(grbg ~ pseq, col = 2)
hist(x$psi_nBn_trainx, freq = FALSE)
lines(grbg ~ pseq, col = 2)
Q
density_list <- mapply(x = prediction_list, c0 = quantile_list, #
                         FUN = .getDensity, SIMPLIFY = FALSE)
c
Q
undebug(.getDensity)
density_list <- mapply(x = prediction_list, c0 = quantile_list, #
                         FUN = .getDensity, SIMPLIFY = FALSE)
density_list
density_list[[1]]
Y_vec <- Reduce(c, lapply(prediction_list, "[[", "test_y"))
Y_vec
Y_vec <- Reduce(c, lapply(prediction_list, "[[", "test_y"))
n <- length(Y_vec)
gn_vec <- rep(mean(Y_vec), n)
gn_vec
F_nBn_y1_at_c0 <- mapply(FUN = function(x,c0){#
    F_nBn_star(psi_x = c0, y = 1, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
  }, c0 = quantile_list, x = prediction_list)
FnBn_y1_at_c0
F_nBn_y1_at_c0
quantile
quantile_list
?quantile
#' @importFrom stats quantile#
.getQuantile <- function(x,p){#
  quantile(x$psi_nBn_trainx, p = p, type = 1)#
}
quantile_list <- lapply(prediction_list, .getQuantile, p = sens)
quantile_list
F_nBn_y1_at_c0 <- mapply(FUN = function(x,c0){#
    F_nBn_star(psi_x = c0, y = 1, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
  }, c0 = quantile_list, x = prediction_list)
F_nBn_y1_at_c0
.getQuantile <- function(x, p){#
  quantile(x$psi_nBn_trainx[x$train_y == 1], p = p, type = 1)#
}
quantile_list <- lapply(prediction_list, .getQuantile, p = sens)
F_nBn_y1_at_c0 <- mapply(FUN = function(x,c0){#
    F_nBn_star(psi_x = c0, y = 1, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
  }, c0 = quantile_list, x = prediction_list)
quantile_list
F_nBn_y1_at_c0
#' Compute CVTML estimates of cross-validated AUC#
#' #
#' TO DO: Add#
#' @param Y The outcome#
#' @param X The predictors#
#' @param K The number of folds#
#' @param learner The learner wrapper#
#' @param seed A random seed to set#
#' @param parallel Compute the predictors in parallel?#
#' @param maxIter Maximum number of iterations for cvtmle#
#' @param icTol Iterate until maxIter is reach or mean of cross-validated#
#' efficient influence function is less than \code{icTol}#
#' @param ... other arguments, not currently used#
#' @importFrom SuperLearner CVFolds#
#' @importFrom cvAUC ci.cvAUC#
#' @importFrom stats uniroot#
#' @export#
#' @return A list#
#' @examples#
#' n <- 200#
#' p <- 10#
#' X <- matrix(rnorm(n*p), nrow = n, ncol = p)#
#' Y <- rbinom(n, 1, plogis(X[,1] + X[,10]))#
#' fit <- cvauc_cvtmle(Y = Y, X = X, K = 5, learner = "glm_wrapper")#
cvauc_cvtmle <- function(Y, X, K, learner = "glm_wrapper", #
                         seed = 1234,#
                         parallel = FALSE, maxIter = 10, #
                         icTol = 1/length(Y), #
                         ...){#
  n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)#
#
  # initial distributions of psi in training samples#
  # dist_psix_y0 <- lapply(prediction_list, .getPsiDistribution, y = 0)#
  # dist_psix_y1 <- lapply(prediction_list, .getPsiDistribution, y = 1)#
  # make long data for targeting step#
  long_data_list <- lapply(prediction_list, .makeLongData, gn = mean(Y))#
  # full_long_data <- Reduce(rbind, long_data_list)#
  # full_long_data$outcome <- with(full_long_data, as.numeric(psi <= u))#
  # full_long_data$logit_Fn <- SuperLearner::trimLogit(full_long_data$Fn, .Machine$double.neg.eps)#
  # targeting#
  epsilon_0 <- rep(0, maxIter)#
  epsilon_1 <- rep(0, maxIter)#
  iter <- 0#
  update_long_data_list <- long_data_list#
  # combine list into data frame#
  full_long_data <- Reduce(rbind, update_long_data_list)#
  # compute mean of EIF#
  D1 <- .Dy(full_long_data, y = 1)#
  D0 <- .Dy(full_long_data, y = 0)#
  ic <- D1 + D0 #
  PnDstar <- mean(ic)#
#
  # compute initial estimate of cvAUC#
  # compute estimated cv-AUC #
  dist_psix_y0_star <- lapply(prediction_list, .getPsiDistribution, #
                         y = 0, epsilon = epsilon_0)#
  dist_psix_y1_star <- lapply(prediction_list, .getPsiDistribution, y = 1,#
                         epsilon = epsilon_1)#
#
  # get AUC#
  init_auc <- mean(mapply(FUN = .getAUC, dist_y0 = dist_psix_y0_star, #
                     dist_y1 = dist_psix_y1_star))#
  est_onestep <- init_auc + PnDstar#
  se_onestep <- sqrt(var(ic)/n)#
#
  # estimating equations#
  est_esteq <- stats::uniroot(.estimatingFn, interval = c(0, 1), #
                   prediction_list = prediction_list, gn = mean(Y))$root#
  se_esteq <- se_onestep#
#
  tmle_auc <- rep(NA, maxIter)#
  PnDstar <- Inf#
  while(PnDstar > icTol & iter < maxIter){#
    iter <- iter + 1#
    # targeting with different epsilon#
    # if(TRUE){#
    ###############
    # target F0#
    ###############
    # make weight for loss function#
    full_long_data$targeting_weight_0 <- #
      as.numeric(full_long_data$Y == 0)/(2*full_long_data$gn) * full_long_data$dFn #
    # fit intercept only model with weights#
    suppressWarnings(#
      fluc_mod_0 <- glm(outcome ~ offset(logit_Fn), family = binomial(),#
                        data = full_long_data[full_long_data$Yi == 0,], #
                        weights = full_long_data$targeting_weight_0[full_long_data$Yi == 0],#
                        start = 0)#
    )#
    epsilon_0[iter] <- as.numeric(fluc_mod_0$coef[1])#
    # if unstable glm fit refit using optim#
    coef_tol <- 1e2#
    if(abs(fluc_mod_0$coef) > coef_tol){#
      fluc_mod_0 <- optim(fluc_mod_optim_0, method = "Brent", par = 0, #
              fld = full_long_data[full_long_data$Yi == 0,],#
              lower = -coef_tol, upper = coef_tol, #
              control = list(reltol = 1e-14))#
      epsilon_0[iter] <- as.numeric(fluc_mod_0$par)#
    }#
    # update values in long_data_list#
    update_long_data_list <- lapply(prediction_list, .makeLongData, gn = mean(Y),#
                             epsilon_0 = epsilon_0, epsilon_1 = epsilon_1,#
                             update = TRUE)#
    # update full long data#
    full_long_data <- Reduce(rbind, update_long_data_list)#
    # sanity check#
    D0_tmp <- c(.Dy(full_long_data, y = 0))#
#
    # make weight for loss function#
    full_long_data$targeting_weight_1 <- #
      as.numeric(full_long_data$Y == 1)/(2*full_long_data$gn) * full_long_data$dFn #
    # fit intercept only model with weights#
    suppressWarnings(#
      fluc_mod_1 <- glm(outcome ~ offset(logit_Fn), family = binomial(),#
                        data = full_long_data[full_long_data$Yi == 1,], #
                        weights = full_long_data$targeting_weight_1[full_long_data$Yi == 1],#
                        start = 0)#
    )#
    # update values in long_data_list#
    epsilon_1[iter] <- as.numeric(fluc_mod_1$coef[1])#
    if(abs(fluc_mod_1$coef) > coef_tol){#
      fluc_mod_1 <- optim(fluc_mod_optim_1, method = "Brent", par = 0, #
              fld = full_long_data[full_long_data$Yi == 1,],#
              lower = -coef_tol, upper = coef_tol, #
              control = list(reltol = 1e-14))#
      epsilon_1[iter] <- as.numeric(fluc_mod_1$par)#
    }#
    update_long_data_list <- lapply(prediction_list, .makeLongData, gn = mean(Y),#
                             epsilon_0 = epsilon_0, epsilon_1 = epsilon_1,#
                             update = TRUE)#
    # update full long data#
    full_long_data <- Reduce(rbind, update_long_data_list)#
    # compute mean of EIF#
    D1 <- .Dy(full_long_data, y = 1)#
    D0 <- .Dy(full_long_data, y = 0)#
    ic <- D1 + D0 #
    PnDstar <- mean(ic)#
#
    # compute estimated cv-AUC #
    dist_psix_y0_star <- lapply(prediction_list, .getPsiDistribution, #
                           y = 0, epsilon = epsilon_0)#
    dist_psix_y1_star <- lapply(prediction_list, .getPsiDistribution, y = 1,#
                           epsilon = epsilon_1)#
#
    # get AUC#
    tmle_auc[iter] <- mean(mapply(FUN = .getAUC, dist_y0 = dist_psix_y0_star, #
                       dist_y1 = dist_psix_y1_star))#
#
  }#
#
    # fluc_mod_0 <- optim(par = 0, fn = .lossF0, long_data_list = long_data_list,#
    #                   method = "Brent", lower = -0.05, upper = 0.05,#
    #                   control = list(reltol = 1e-9))#
    # ic_0 <- .D0(long_data_list, epsilon = fluc_mod_0$par)#
    # fluc_mod_1 <- optim(par = 0, fn = .lossF1, long_data_list = long_data_list,#
    #                   method = "Brent", lower = -0.05, upper = 0.05,#
    #                   control = list(reltol = 1e-9))#
    # ic_1 <- .D1(long_data_list, epsilon = fluc_mod_1$par)#
    # compute regular cvAUC#
    valid_pred_list <- lapply(prediction_list, "[[", "psi_nBn_testx")#
    valid_label_list <- lapply(prediction_list, "[[", "test_y")#
    regular_cvauc <- cvAUC::ci.cvAUC(predictions = valid_pred_list,#
                                labels = valid_label_list)#
    est_empirical <- regular_cvauc$cvAUC#
    se_empirical <- regular_cvauc$se#
    # # true CV AUC#
    # N <- 1e5#
    # p <- 100#
    # bigX <- matrix(rnorm(N*p), nrow = N, ncol = p)#
    # bigY <- rbinom(N, 1, plogis(bigX[,1] + bigX[,10] + bigX[,20]))#
    # big_valid_pred_list <- lapply(prediction_list, function(x){#
    #   predict(x$model, newx = bigX, type = "response")#
    # })#
    # big_label_list <- list(bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY,#
    #                        bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY,bigY)#
    # true_cvauc <- mean(cvAUC::AUC(predictions = big_valid_pred_list,#
    #                         labels = big_label_list))#
#
    # format output#
    out <- list()#
    out$est_cvtmle <- tmle_auc[iter]#
    out$iter_cvtmle <- iter#
    out$cvtmle_trace <- tmle_auc#
    out$se_cvtmle <- sqrt(var(ic)/n)#
    out$est_init <- init_auc#
    out$est_empirical <- est_empirical#
    out$se_empirical <- se_empirical#
    out$est_onestep <- est_onestep#
    out$se_onestep <- se_onestep#
    out$est_esteq <- est_esteq#
    out$se_esteq <- se_esteq#
#
    out$models <- lapply(prediction_list, "[[", "model")#
    class(out) <- "cvauc"#
    return(out)#
}#
#
#' Alternative fluctuation routine #
#' @param epsilon Fluctuation parameter #
#' @param fld full_long_data_list#
fluc_mod_optim_0 <- function(epsilon, fld, tol = 1e-3){#
  p_eps <- plogis(fld$logit_Fn + epsilon)#
  p_eps[p_eps == 1] <- 1 - tol#
  p_eps[p_eps == 0] <- tol#
  loglik <- -sum(fld$targeting_weight_0 * (fld$outcome * log(p_eps) + (1-fld$outcome) * log(1 - p_eps)))#
  return(loglik)#
}#
#' Alternative fluctuation routine #
#' @param epsilon Fluctuation parameter #
#' @param fld full_long_data_list#
fluc_mod_optim_1 <- function(epsilon, fld, tol = 1e-3){#
  p_eps <- plogis(fld$logit_Fn + epsilon)#
  p_eps[p_eps == 1] <- 1 - tol#
  p_eps[p_eps == 0] <- tol#
  loglik <- -sum(fld$targeting_weight_1 * (fld$outcome * log(p_eps) + (1-fld$outcome) * log(1 - p_eps)))#
  return(loglik)#
}#
#
#' An estimating function for cvAUC#
#' @param auc The value of auc to find root for#
#' @param prediction_list Entry in prediction_list#
#' @param gn Marginal probability of outcome#
.estimatingFn <- function(auc = 0.5, prediction_list, gn){#
  # get first influence function piece for everyone#
  ic_1 <- #
  Reduce("c",lapply(prediction_list, function(x){#
    thisFn <- sapply(1:length(x$test_y), function(i){#
      ifelse(x$test_y[i] == 1, #
             F_nBn_star(x$psi_nBn_testx[i], y = 0, Psi_nBn_0 = x$psi_nBn_trainx,#
                        Y_Bn = x$train_y)/ gn , #
             (1 - F_nBn_star(x$psi_nBn_testx[i], y = 1, Psi_nBn_0 = x$psi_nBn_trainx,#
                        Y_Bn = x$train_y))/(1 - gn))#
    })#
  }))#
  all_y <- unlist(lapply(prediction_list, "[[", "test_y"))#
  ic_2 <- rep(0, length(all_y))#
  ic_2[all_y == 0] <- - auc / (1 - gn)#
  ic_2[all_y == 1] <- - auc / gn#
  return(mean(ic_1 + ic_2))#
}#
#
# uniroot(.estimatingFn, interval = c(0, 1), prediction_list = prediction_list, gn = mean(Y))#
#
#' Compute a portion of the efficient influence function#
#' @param full_long_data A long form data set#
#' @param y Which portion of the EIF to compute#
#' @return Vector of EIF#
.Dy <- function(full_long_data, y){#
  by(full_long_data, full_long_data$id, function(x){#
    sum((-1)^y * as.numeric(x$Y == y)/(x$gn) * (x$outcome - x$Fn) * x$dFn)#
  })#
}#
#
#' Compute the (CV)TMLE cumulative dist at psi_x#
#' @param psi_x Value to compute conditional (on Y=y) cdf of Psi#
#' @param y Value of Y to condition on #
#' @param Psi_nBn_0 Values of Psi_nBn(X) from training sample#
#' @param Y_Bn Values of Y from training sample#
#' @param epsilon Vector of fluctuation parameter estimates#
#' @return Numeric value of CDF#
F_nBn_star <- function(psi_x, y, Psi_nBn_0, Y_Bn, epsilon = 0, #
                       # tol = .Machine$double.neg.eps#
                       tol = 1e-3#
                       ){#
  plogis(SuperLearner::trimLogit(mean(Psi_nBn_0[Y_Bn %in% y] <= psi_x), tol) +#
          sum(epsilon))#
}#
#
#' Worker function to make long form data set needed for#
#' CVTMLE targeting step #
#' #
#' @param x An entry in the "predictions list" that has certain#
#' named values (see \code{?.getPredictions})#
#' @param gn An estimate of the marginal dist. of Y#
#' @param update Boolean of whether this is called for initial#
#' construction of the long data set or as part of the targeting loop. #
#' If the former, empirical "density" estimates are used. If the latter#
#' these are derived from the targeted cdf. #
#' @param epsilon_0 If \code{update = TRUE}, a vector of TMLE fluctuation#
#' parameter estimates used to add the CDF and PDF of Psi(X) to the data set#
#' @param epsilon_1 Ditto above#
#' #
#' @return A long form data list of a particular set up. Columns are named id #
#' (multiple per obs. in validation sample), u (if Yi = 0, these are the values of psi(x) in the#
#' training sample for obs with Y = 1, if Yi = 1, these are values of psi(x) in#
#' the training sample for obs. with Y = 0), Yi (this id's value of Y), Fn (#
#' estimated value of the cdf of psi(X) given Y = Yi in the training sample), #
#' dFn (estimated value of the density of psi(X) given Y = (1-Yi) in the #
#' training sample), psi (the value of this observations \hat{\Psi}(P_{n,B_n}^0)),#
#' gn (estimate of marginal of Y e.g., computed in whole sample), outcome (indicator#
#' that psix <= u), logit_Fn (the cdf estimate on the logit scale, needed for #
#' offset in targeting model).#
#
.makeLongData <- function(x, gn, update = FALSE, epsilon_0 = 0, epsilon_1 = 0,#
                          # tol = .Machine$double.neg.eps, #
                          tol = 1e-3#
                          ){#
  # first the dumb way, writing a loop over x$psi_nBn_testx#
  uniq_train_psi_y0 <- sort(unique(x$psi_nBn_trainx[x$train_y == 0]))#
  uniq_train_psi_y1 <- sort(unique(x$psi_nBn_trainx[x$train_y == 1]))#
  # ord_train_psi_y0 <- order(x$psi_nBn_trainx[x$train_y == 0])#
  # ord_train_psi_y1 <- order(x$psi_nBn_trainx[x$train_y == 1])#
  n_valid <- length(x$psi_nBn_testx)#
  n_train <- length(x$psi_nBn_trainx)#
  n1_train <- sum(x$train_y)#
  n1_uniq_train <- length(uniq_train_psi_y1)#
  n0_train <- n_train - n1_train#
  n0_uniq_train <- length(uniq_train_psi_y0)#
  n1_valid <- sum(x$test_y)#
  n0_valid <- n_valid - n1_valid#
  valid_ids <- as.numeric(names(x$psi_nBn_testx))#
  tot_length <- n1_valid * n0_uniq_train + n0_valid * n1_uniq_train#
#
  idVec <- rep(NA, tot_length)#
  uVec <- rep(NA, tot_length)#
  YuVec <- rep(NA, tot_length)#
  YiVec <- rep(NA, tot_length)#
  F_1n_Bn_uVec <- rep(NA, tot_length)#
  F_0n_Bn_uVec <- rep(NA, tot_length)#
  dF_1n_Bn_uVec <- rep(NA, tot_length)#
  dF_0n_Bn_uVec <- rep(NA, tot_length)#
  FnVec <- rep(NA, tot_length)#
  dFnVec <- rep(NA, tot_length)#
  psiVec <- rep(NA, tot_length)#
#
  # cumulative dist of psi_n | Y = 1 evaluated at all values of #
  # psi_n associated with Y = 0 observations#
  F1nBn <- sapply(uniq_train_psi_y0, #
                  F_nBn_star, Psi_nBn_0 = x$psi_nBn_trainx, y = 1, #
                  Y_Bn = x$train_y, epsilon = epsilon_1)#
  # cumulative dist of psi_n | Y = 0 evaluated at all values of #
  # psi_n associated with Y = 1 observations#
  F0nBn <- sapply(uniq_train_psi_y1, #
                  F_nBn_star, Psi_nBn_0 = x$psi_nBn_trainx, y = 0, #
                  Y_Bn = x$train_y, epsilon = epsilon_0)#
#
  # empirical dens of psi_n | Y = 1 and Y = 0 evaluated at all #
  if(!update){#
    dF1nBn <- as.numeric(table(x$psi_nBn_trainx[x$train_y == 1])/n1_train)#
    dF0nBn <- as.numeric(table(x$psi_nBn_trainx[x$train_y == 0])/n0_train)#
  }else{#
    # cumulative dist of psi_n | Y = 1 evaluated at all values of #
    # psi_n associated with Y = 0 observations#
    F1nBn_1 <- sapply(uniq_train_psi_y1, #
                    F_nBn_star, Psi_nBn_0 = x$psi_nBn_trainx, y = 1, #
                    Y_Bn = x$train_y, epsilon = epsilon_1)#
    dF1nBn <- diff(c(0, F1nBn_1))#
    # cumulative dist of psi_n | Y = 0 evaluated at all values of #
    # psi_n associated with Y = 1 observations#
    F0nBn_0 <- sapply(uniq_train_psi_y0, #
                    F_nBn_star, Psi_nBn_0 = x$psi_nBn_trainx, y = 0, #
                    Y_Bn = x$train_y, epsilon = epsilon_0)#
    dF0nBn <- diff(c(0, F0nBn_0))#
  }#
#
  # loop over folks in validation fold#
  cur_start <- 1#
  for(i in seq_len(n_valid)){#
    if(x$test_y[i] == 0){#
      cur_end <- cur_start + n1_uniq_train - 1#
      idVec[cur_start:cur_end] <- x$valid_ids[i]#
      # ordered unique values of psi in training | y = 1#
      uVec[cur_start:cur_end] <- uniq_train_psi_y1#
      # value of this Y_i#
      YiVec[cur_start:cur_end] <- 0#
      # cdf of psi | y = 0 in training at each u#
      FnVec[cur_start:cur_end] <- F0nBn#
      # pdf of psi | y = 1 in training at each u#
      dFnVec[cur_start:cur_end] <- dF1nBn#
      # vector of this psi#
      psiVec[cur_start:cur_end] <- x$psi_nBn_testx[i]#
    }else{#
      cur_end <- cur_start + n0_uniq_train - 1#
      idVec[cur_start:cur_end] <- x$valid_ids[i]#
      # ordered unique values of psi in training | y = 0#
      uVec[cur_start:cur_end] <- uniq_train_psi_y0#
      # value of this Y_i#
      YiVec[cur_start:cur_end] <- 1#
      # cdf of psi | y = 1 in training at each u#
      FnVec[cur_start:cur_end] <- F1nBn#
      # pdf of psi | y = 0 in training at each u#
      dFnVec[cur_start:cur_end] <- dF0nBn#
      # vector of this psi#
      psiVec[cur_start:cur_end] <- x$psi_nBn_testx[i]#
    }#
    cur_start <- cur_end + 1#
  }#
#
  out <- data.frame(id = idVec, u = uVec,#
                   Yi = YiVec, Fn = FnVec, dFn = dFnVec,#
                   psi = psiVec)#
#
  # add in gn#
  out$gn <- NA#
  out$gn[out$Yi == 1] <- gn#
  out$gn[out$Yi == 0] <- 1 - gn#
  # add in "outcome"#
  out$outcome <- as.numeric(out$psi <= out$u)#
  # add in logit(Fn)#
  out$logit_Fn <- SuperLearner::trimLogit(out$Fn, tol)#
  return(out)#
}#
#' Compute the AUC given the cdf and pdf of psi #
#' #
#' See \code{?.getPsiDistribution} to understand expected input format#
#' #
#' @param dist_y0 Distribution of psi given Y = 0#
#' @param dist_y1 Distribution of psi given Y = 1#
#' @return Numeric #
# TO DO: make more efficient#
# TO DO: how are ties handled in findInterval?#
.getAUC <- function(dist_y0, dist_y1){#
  tot <- 0#
  for(i in seq_along(dist_y0$psix)){#
    idx <- findInterval(x = dist_y0$psix[i], vec = dist_y1$psix)#
    p1 <- ifelse(idx == 0, 1, (1 - dist_y1$Fn[idx]))#
    p2 <- dist_y0$dFn[i]#
    tot <- tot + p1 * p2#
  }#
  return(tot)#
}#
#
#' Compute the conditional (given Y = y) estimated distribution of psi#
#' #
#' @param x An entry in the output from .getPredictions#
#' @param y What value of Y to compute dist. est. #
#' @param epsilon A vector of estimated coefficients form tmle fluctuation #
#' submodels. #
#' #
#' @return A data.frame with the distribution of psi given Y = y with names#
#' psix (what value estimates are evaluated at), dFn (density estimates),#
#' Fn (cdf estimates)#
.getPsiDistribution <- function(x, y, epsilon = 0){#
    this_n <- length(x$psi_nBn_trainx[x$train_y == y])#
    uniq_train_psi_y <- sort(unique(x$psi_nBn_trainx[x$train_y == y]))#
    FynBn_y <- sapply(uniq_train_psi_y, #
                F_nBn_star, Psi_nBn_0 = x$psi_nBn_trainx, y = y, #
                Y_Bn = x$train_y, epsilon = epsilon)#
    dFynBn <- diff(c(0, FynBn_y))#
    out <- data.frame(psix = uniq_train_psi_y, #
                      dFn = dFynBn,#
                      Fn = FynBn_y)#
    return(out)#
}#
#' Worker function for fitting prediction functions (possibly in parallel)#
#' #
#' @param learner The wrapper to use#
#' @param Y The outcome#
#' @param X The predictors#
#' @param K The number of folds#
#' @param parallel Whether to compute things in parallel using future#
#' #
#' @return A list of the result of the wrapper executed in each fold#
.getPredictions <- function(learner, Y, X, K, folds, parallel){#
#
  .doFit <- function(x, tmpX, Y, folds, learner){#
    out <- do.call(learner, args=list(train = list(Y = Y[-folds[[x]]], X = tmpX[-folds[[x]],,drop=FALSE]),#
                                      test = list(Y = Y[folds[[x]]], X = tmpX[folds[[x]],,drop=FALSE])))#
    out$valid_ids <- folds[[x]]#
    return(out)#
  }#
#
  if(parallel){#
    stop("Parallel processing code needs to be re-written.")#
    # cl <- makeCluster(detectCores())#
    # registerDoParallel(cl)#
    # predFitList <- foreach(v = 1:length(folds), .export=learner) %dopar% #
    #   .doFit(v, tmpX = X, Y = Y, folds = folds, learner = learner)#
    # stopCluster(cl)#
  }else{#
    predFitList <- lapply(split(seq(K),factor(seq(K))),FUN = .doFit, tmpX = X, Y = Y, folds = folds, learner = learner)#
  }#
  # return results#
  return(predFitList)#
}
F_nBn_marg_at_c0 <- mapply(FUN = function(x,c0){#
    F_nBn_star(psi_x = c0, y = c(0,1), Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
  }, c0 = quantile_list, x = prediction_list)
F_nBn_marg_at_c0
F_nBn <- mapply(FUN = function(x,c0){#
    F_nBn_y1_at_c0 <- F_nBn_star(psi_x = c0, y = 1, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    F_nBn_y0_at_c0 <- F_nBn_star(psi_x = c0, y = 0, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    ifelse(x$test_y == 0, F_nBn_y0_at_c0, F_nBn_y1_at_c0)#
  }, c0 = quantile_list, x = prediction_list)
F_nBn
prediction_list[[1]]$testy
prediction_list[[1]]$test_y
Y_vec <- Reduce(c, lapply(prediction_list, "[[", "test_y"))
n <- length(Y_vec)
gn_vec <- rep(mean(Y_vec), n)
F_nBn_vec <- Reduce(c,mapply(FUN = function(x,c0){#
    F_nBn_y1_at_c0 <- F_nBn_star(psi_x = c0, y = 1, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    F_nBn_y0_at_c0 <- F_nBn_star(psi_x = c0, y = 0, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    ifelse(x$test_y == 0, F_nBn_y0_at_c0, F_nBn_y1_at_c0)#
  }, c0 = quantile_list, x = prediction_list))
F_nBn_vec
density_list
dens_ratio <- Reduce(c, mapply(FUN = function(x, dens){#
    rep(density_list[[2]]/density_list[[1]], length(x$test_y))#
  }, x = prediction_list, dens = density_list))
dens_ratio <- Reduce(c, mapply(FUN = function(x, dens){#
    rep(dens[[2]]/dens[[1]], length(x$test_y))#
  }, x = prediction_list, dens = density_list))
dens_ratio
ind <- Reduce(c, mapply(x = prediction_list, c0 = quantile_list, function(x, c0){#
    as.numeric(x$psi_nBn_testx <= c0)#
  }))
ind
out <- data.frame(Y = Y_vec, gn = gn_vec, Fn = F_nBn_vec,#
                    f_ratio = dens_ratio, ind = ind)
out
.makeTargetingData <- function(prediction_list, quantile_list, density_list){#
  Y_vec <- Reduce(c, lapply(prediction_list, "[[", "test_y"))#
  n <- length(Y_vec)#
  gn_vec <- rep(mean(Y_vec), n)#
  F_nBn_vec <- Reduce(c,mapply(FUN = function(x,c0){#
    F_nBn_y1_at_c0 <- F_nBn_star(psi_x = c0, y = 1, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    F_nBn_y0_at_c0 <- F_nBn_star(psi_x = c0, y = 0, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    ifelse(x$test_y == 0, F_nBn_y0_at_c0, F_nBn_y1_at_c0)#
  }, c0 = quantile_list, x = prediction_list))#
  dens_ratio <- Reduce(c, mapply(FUN = function(x, dens){#
    rep(dens[[2]]/dens[[1]], length(x$test_y))#
  }, x = prediction_list, dens = density_list))#
  ind <- Reduce(c, mapply(x = prediction_list, c0 = quantile_list, function(x, c0){#
    as.numeric(x$psi_nBn_testx <= c0)#
  }))#
#
  out <- data.frame(Y = Y_vec, gn = gn_vec, Fn = F_nBn_vec,#
                    f_ratio = dens_ratio, ind = ind)#
  return(out)#
}
target_data <- .makeTargetingData(prediction_list, quantile_list, density_list)
head(target_data)
target_data$weight <- with(target_data, 1 + Y/gn * f_ratio)
head(target_data)
target_data
names(target_data)
fluc_mod <- glm(ind ~ offset(logit_Fn), data = target_data, family = "binomial")
target_data$logit_Fn <- SuperLearner::trimLogit(target_data$Fn, tol = 1e-3)
fluc_mod <- glm(ind ~ offset(logit_Fn), data = target_data, family = "binomial")
SuperLearner::trimLogit
target_data$logit_Fn <- SuperLearner::trimLogit(target_data$Fn, trim = 1e-5)
fluc_mod <- glm(ind ~ offset(logit_Fn), data = target_data, family = "binomial")
fluc_mod
names(fluc_mod)
table(fluc_mod$fitted.values)
fluc_mod <- glm(ind ~ offset(logit_Fn), data = target_data, family = "binomial",#
                  weights = weight)
fluc_mod
fluc_mod$fitted.values
target_data$Y
table(fluc_mod$fitted.values)
folds
lapply(folds, length)
.makeTargetingData <- function(prediction_list, quantile_list, density_list){#
  Y_vec <- Reduce(c, lapply(prediction_list, "[[", "test_y"))#
  n <- length(Y_vec)#
  fold_vec <- rep(0, n)#
  for(i in seq_along(folds)) fold_vec[folds[[i]]] <- i#
  gn_vec <- rep(mean(Y_vec), n)#
  F_nBn_vec <- Reduce(c,mapply(FUN = function(x,c0){#
    F_nBn_y1_at_c0 <- F_nBn_star(psi_x = c0, y = 1, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    F_nBn_y0_at_c0 <- F_nBn_star(psi_x = c0, y = 0, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    ifelse(x$test_y == 0, F_nBn_y0_at_c0, F_nBn_y1_at_c0)#
  }, c0 = quantile_list, x = prediction_list))#
  dens_ratio <- Reduce(c, mapply(FUN = function(x, dens){#
    rep(dens[[2]]/dens[[1]], length(x$test_y))#
  }, x = prediction_list, dens = density_list))#
  ind <- Reduce(c, mapply(x = prediction_list, c0 = quantile_list, function(x, c0){#
    as.numeric(x$psi_nBn_testx <= c0)#
  }))#
  out <- data.frame(fold = fold_vec, Y = Y_vec, gn = gn_vec, Fn = F_nBn_vec,#
                    f_ratio = dens_ratio, ind = ind)#
  return(out)#
}
target_data <- .makeTargetingData(prediction_list, quantile_list, density_list, folds)
.makeTargetingData <- function(prediction_list, quantile_list, density_list){#
  Y_vec <- Reduce(c, lapply(prediction_list, "[[", "test_y"))#
  n <- length(Y_vec)#
  fold_vec <- rep(0, n)#
  for(i in seq_along(folds)) fold_vec[folds[[i]]] <- i#
  gn_vec <- rep(mean(Y_vec), n)#
  F_nBn_vec <- Reduce(c,mapply(FUN = function(x,c0){#
    F_nBn_y1_at_c0 <- F_nBn_star(psi_x = c0, y = 1, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    F_nBn_y0_at_c0 <- F_nBn_star(psi_x = c0, y = 0, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    ifelse(x$test_y == 0, F_nBn_y0_at_c0, F_nBn_y1_at_c0)#
  }, c0 = quantile_list, x = prediction_list))#
  dens_ratio <- Reduce(c, mapply(FUN = function(x, dens){#
    rep(dens[[2]]/dens[[1]], length(x$test_y))#
  }, x = prediction_list, dens = density_list))#
  ind <- Reduce(c, mapply(x = prediction_list, c0 = quantile_list, function(x, c0){#
    as.numeric(x$psi_nBn_testx <= c0)#
  }))#
  out <- data.frame(fold = fold_vec, Y = Y_vec, gn = gn_vec, Fn = F_nBn_vec,#
                    f_ratio = dens_ratio, ind = ind)#
  return(out)#
}
target_data <- .makeTargetingData(prediction_list, quantile_list, density_list, folds)
.makeTargetingData <- function(prediction_list, quantile_list, density_list, folds){#
  Y_vec <- Reduce(c, lapply(prediction_list, "[[", "test_y"))#
  n <- length(Y_vec)#
  fold_vec <- rep(0, n)#
  for(i in seq_along(folds)) fold_vec[folds[[i]]] <- i#
  gn_vec <- rep(mean(Y_vec), n)#
  F_nBn_vec <- Reduce(c,mapply(FUN = function(x,c0){#
    F_nBn_y1_at_c0 <- F_nBn_star(psi_x = c0, y = 1, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    F_nBn_y0_at_c0 <- F_nBn_star(psi_x = c0, y = 0, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    ifelse(x$test_y == 0, F_nBn_y0_at_c0, F_nBn_y1_at_c0)#
  }, c0 = quantile_list, x = prediction_list))#
  dens_ratio <- Reduce(c, mapply(FUN = function(x, dens){#
    rep(dens[[2]]/dens[[1]], length(x$test_y))#
  }, x = prediction_list, dens = density_list))#
  ind <- Reduce(c, mapply(x = prediction_list, c0 = quantile_list, function(x, c0){#
    as.numeric(x$psi_nBn_testx <= c0)#
  }))#
  out <- data.frame(fold = fold_vec, Y = Y_vec, gn = gn_vec, Fn = F_nBn_vec,#
                    f_ratio = dens_ratio, ind = ind)#
  return(out)#
}
target_data <- .makeTargetingData(prediction_list, quantile_list, density_list, folds)
target_data
.makeTargetingData <- function(prediction_list, quantile_list, density_list, folds){#
  Y_vec <- Reduce(c, lapply(prediction_list, "[[", "test_y"))#
  n <- length(Y_vec)#
  fold_vec <- sort(rep(1:length(folds), unlist(lapply(folds, length))))#
  gn_vec <- rep(mean(Y_vec), n)#
  F_nBn_vec <- Reduce(c,mapply(FUN = function(x,c0){#
    F_nBn_y1_at_c0 <- F_nBn_star(psi_x = c0, y = 1, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    F_nBn_y0_at_c0 <- F_nBn_star(psi_x = c0, y = 0, Psi_nBn_0 = x$psi_nBn_trainx, Y_Bn = x$train_y)#
    ifelse(x$test_y == 0, F_nBn_y0_at_c0, F_nBn_y1_at_c0)#
  }, c0 = quantile_list, x = prediction_list))#
  dens_ratio <- Reduce(c, mapply(FUN = function(x, dens){#
    rep(dens[[2]]/dens[[1]], length(x$test_y))#
  }, x = prediction_list, dens = density_list))#
  ind <- Reduce(c, mapply(x = prediction_list, c0 = quantile_list, function(x, c0){#
    as.numeric(x$psi_nBn_testx <= c0)#
  }))#
  out <- data.frame(fold = fold_vec, Y = Y_vec, gn = gn_vec, Fn = F_nBn_vec,#
                    f_ratio = dens_ratio, ind = ind)#
  return(out)#
}
target_data <- .makeTargetingData(prediction_list, quantile_list, density_list, folds)
target_data
target_data$weight <- with(target_data, 1 + Y/gn * f_ratio)
target_data$logit_Fn <- SuperLearner::trimLogit(target_data$Fn, trim = 1e-5)
fluc_mod <- glm(ind ~ offset(logit_Fn), data = target_data, family = "binomial",
weights = weight)
fluc_mod
target_data$Fnstar <- fluc_mod$fitted.values
target_data
by(target_data, target_data$fold, function(x){#
    x$Fn[x$Y==0][1] * (1-gn) + x$Fn[x$Y==1] * gn#
  })
by(target_data, target_data$fold, function(x){#
    x$Fn[x$Y==0][1] * (1-x$gn) + x$Fn[x$Y==1] * x$gn#
  })
by(target_data, target_data$fold, function(x){#
    x$Fn[x$Y==0][1] * (1-x$gn[1]) + x$Fn[x$Y==1][1] * x$gn[1]#
  })
sens <- 0.8
n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)#
  # get quantile estimates#
  quantile_list <- lapply(prediction_list, .getQuantile, p = sens)#
#
  # get density estimate #
  density_list <- mapply(x = prediction_list, c0 = quantile_list, #
                         FUN = .getDensity, SIMPLIFY = FALSE)#
#
  # make targeting data#
  target_data <- .makeTargetingData(prediction_list, quantile_list, density_list, folds)#
  target_data$weight <- with(target_data, 1 + Y/gn * f_ratio)#
  target_data$logit_Fn <- SuperLearner::trimLogit(target_data$Fn, trim = 1e-5)#
  fluc_mod <- glm(ind ~ offset(logit_Fn), data = target_data, family = "binomial",#
                  weights = weight)#
  target_data$Fnstar <- fluc_mod$fitted.values#
#
  # compute parameter for each fold#
  cv_estimates <- by(target_data, target_data$fold, function(x){#
    x$Fn[x$Y==0][1] * (1-x$gn[1]) + x$Fn[x$Y==1][1] * x$gn[1]#
  })
cv_estimates
gn
head(target_data)
sens
(0.8 * 0.54) / (1 - mean(cv_estimates))
mean(cv_estimates)
1 - mean(cv_estimates)
0.8 * 0.54
n <- 200
p <- 5
X <- data.frame(matrix(rnorm(n*p), nrow = n, ncol = p))
Y <- rbinom(n, 1, plogis(X[,1] + X[,2]))
K <- 5
learner <- "glm_wrapper"
n <- length(Y)#
  set.seed(seed)#
  folds <- SuperLearner::CVFolds(N = n, id = NULL, Y = Y, #
                                 cvControl = list(V = K, stratifyCV = TRUE, #
                                    shuffle = TRUE, validRows = NULL))#
  prediction_list <- .getPredictions(learner = learner, Y = Y, X = X, #
                                 K = K, folds=folds, parallel = FALSE)#
  # get quantile estimates#
  quantile_list <- lapply(prediction_list, .getQuantile, p = sens)#
#
  # get density estimate #
  density_list <- mapply(x = prediction_list, c0 = quantile_list, #
                         FUN = .getDensity, SIMPLIFY = FALSE)#
#
  # make targeting data#
  target_data <- .makeTargetingData(prediction_list, quantile_list, density_list, folds)#
  target_data$weight <- with(target_data, 1 + Y/gn * f_ratio)#
  target_data$logit_Fn <- SuperLearner::trimLogit(target_data$Fn, trim = 1e-5)#
  fluc_mod <- glm(ind ~ offset(logit_Fn), data = target_data, family = "binomial",#
                  weights = weight)#
  target_data$Fnstar <- fluc_mod$fitted.values#
#
  # compute parameter for each fold#
  cv_estimates <- by(target_data, target_data$fold, function(x){#
    x$Fn[x$Y==0][1] * (1-x$gn[1]) + x$Fn[x$Y==1][1] * x$gn[1]#
  })
cv_estimates
mean(cv_estimates)
target_data
sample(1:10)
sample(1:10, replacement = TRUE)
sample(1:10, replac = TRUE)
sample(1:10, replace = TRUE)
# Let's write a function that gives us a bootstrap resample. #
# Recall in class that we showed that sampling from the empirical#
# distribution is equivalent to sampling with replacement from the data.#
get_boot_stample <- function(one_sample){#
    # compute n#
    n <- length(one_sample)#
#
    # sample a vector of numbers from 1:n with replacement#
    ids_in_this_bootsample <- sample(1:n, replace = TRUE)#
#
    # bootstrap sample is one_sample indexed by this vector#
    boot_sample <- one_sample[ids_in_this_bootsample]#
    # return these observations#
    return(boot_sample)#
}#
#
# Now let's write a function that will compute a bootstrap#
# confidence interval based on B resamples using Method 1#
# discussed in class (Wald-style bootstrap interval)#
compute_normal_boot_ci <- function(one_sample, B = 1e2){#
    # The form of this confidence interval is lambda_n +/- 1.96 * se_boot#
    # Let's first compute the MLE#
    lambda_n <- compute_xbar(one_sample)#
#
    # Now we need a bootstrap estimate of it's standard error. #
    # To do this we repeatedly generate bootstrap samples and #
    # compute the MLE each time. Let's write a function that does #
    # this and returns a vector of bootstrapped MLEs#
    .bootstrap_lambda <- function(one_sample){#
        # get a bootstrap sample#
        boot_sample <- get_boot_stample(one_sample)#
        # compute MLE based on boot sample#
        lambda_n_star <- compute_xbar(boot_sample)#
        return(lambda_n_star)#
    }#
#
    # Now we replicate this function B times#
    lambda_n_star_vector <- replicate(B, .bootstrap_lambda(one_sample))#
#
    # The bootstrap standard error estimate is the sd of the#
    # bootstrapped MLEs#
    se_boot <- sd(lambda_n_star_vector)#
#
    # The confidence interval #
    ci <- c(lambda_n - qlogis(0.975) * se_boot,#
            lambda_n + qlogis(0.975) * se_boot)#
#
    # Return the CI#
    return(ci)#
}#
#
# Try it out!#
test_sample <- get_one_sample(n = 50)#
compute_normal_boot_ci(test_sample)
#--------------------------------------------#
# Demonstration of bootstrap #
#--------------------------------------------#
# We'll reuse some functions from last time (copied here).#
#
#--------------------------#
# start of old functions #
#--------------------------#
# A function to simulate a sample of independent#
# observations from a Poisson(lambda = 2) population#
grab_one_sample <- function(n, lambda_0 = 2){#
    x_vec <- rpois(n, lambda_0)#
    return(x_vec)#
}#
#
# A function to compute a statistic given a sample.#
# In this case, we compute the mean#
compute_xbar <- function(one_sample){#
    xbar <- mean(one_sample)#
    return(xbar)#
}#
#
# Our "recipe" to compute a Wald-style confidence interval given a #
# sample of data. #
compute_ci <- function(one_sample){#
    # sample size#
    n <- length(one_sample)#
    # MLE#
    xbar <- mean(one_sample)#
    # Standard error estimate#
    se <- sqrt(xbar / n)#
    # quantile of standard normal#
    z_alphaover2 <- qnorm(0.975)#
    # confidence interval#
    ci <- c(xbar - z_alphaover2 * se,#
            xbar + z_alphaover2 * se)#
    return(ci)#
}#
#
#--------------------------#
# end of old functions #
#--------------------------#
#
# Let's write a function that gives us a bootstrap resample. #
# Recall in class that we showed that sampling from the empirical#
# distribution is equivalent to sampling with replacement from the data.#
get_boot_stample <- function(one_sample){#
    # compute n#
    n <- length(one_sample)#
#
    # sample a vector of numbers from 1:n with replacement#
    ids_in_this_bootsample <- sample(1:n, replace = TRUE)#
#
    # bootstrap sample is one_sample indexed by this vector#
    boot_sample <- one_sample[ids_in_this_bootsample]#
    # return these observations#
    return(boot_sample)#
}#
#
# Now let's write a function that will compute a bootstrap#
# confidence interval based on B resamples using Method 1#
# discussed in class (Wald-style bootstrap interval)#
compute_normal_boot_ci <- function(one_sample, B = 1e2){#
    # The form of this confidence interval is lambda_n +/- 1.96 * se_boot#
    # Let's first compute the MLE#
    lambda_n <- compute_xbar(one_sample)#
#
    # Now we need a bootstrap estimate of it's standard error. #
    # To do this we repeatedly generate bootstrap samples and #
    # compute the MLE each time. Let's write a function that does #
    # this and returns a vector of bootstrapped MLEs#
    .bootstrap_lambda <- function(one_sample){#
        # get a bootstrap sample#
        boot_sample <- get_boot_stample(one_sample)#
        # compute MLE based on boot sample#
        lambda_n_star <- compute_xbar(boot_sample)#
        return(lambda_n_star)#
    }#
#
    # Now we replicate this function B times#
    lambda_n_star_vector <- replicate(B, .bootstrap_lambda(one_sample))#
#
    # The bootstrap standard error estimate is the sd of the#
    # bootstrapped MLEs#
    se_boot <- sd(lambda_n_star_vector)#
#
    # The confidence interval #
    ci <- c(lambda_n - qlogis(0.975) * se_boot,#
            lambda_n + qlogis(0.975) * se_boot)#
#
    # Return the CI#
    return(ci)#
}#
#
# Try it out!#
test_sample <- get_one_sample(n = 50)#
compute_normal_boot_ci(test_sample)
#--------------------------------------------#
# Demonstration of bootstrap #
#--------------------------------------------#
# We'll reuse some functions from last time (copied here).#
#
#--------------------------#
# start of old functions #
#--------------------------#
# A function to simulate a sample of independent#
# observations from a Poisson(lambda = 2) population#
grab_one_sample <- function(n, lambda_0 = 2){#
    x_vec <- rpois(n, lambda_0)#
    return(x_vec)#
}#
#
# A function to compute a statistic given a sample.#
# In this case, we compute the mean#
compute_xbar <- function(one_sample){#
    xbar <- mean(one_sample)#
    return(xbar)#
}#
#
# Our "recipe" to compute a Wald-style confidence interval given a #
# sample of data. #
compute_ci <- function(one_sample){#
    # sample size#
    n <- length(one_sample)#
    # MLE#
    xbar <- mean(one_sample)#
    # Standard error estimate#
    se <- sqrt(xbar / n)#
    # quantile of standard normal#
    z_alphaover2 <- qnorm(0.975)#
    # confidence interval#
    ci <- c(xbar - z_alphaover2 * se,#
            xbar + z_alphaover2 * se)#
    return(ci)#
}#
#
#--------------------------#
# end of old functions #
#--------------------------#
#
# Let's write a function that gives us a bootstrap resample. #
# Recall in class that we showed that sampling from the empirical#
# distribution is equivalent to sampling with replacement from the data.#
get_boot_stample <- function(one_sample){#
    # compute n#
    n <- length(one_sample)#
#
    # sample a vector of numbers from 1:n with replacement#
    ids_in_this_bootsample <- sample(1:n, replace = TRUE)#
#
    # bootstrap sample is one_sample indexed by this vector#
    boot_sample <- one_sample[ids_in_this_bootsample]#
    # return these observations#
    return(boot_sample)#
}#
#
# Now let's write a function that will compute a bootstrap#
# confidence interval based on B resamples using Method 1#
# discussed in class (Wald-style bootstrap interval)#
compute_normal_boot_ci <- function(one_sample, B = 1e2){#
    # The form of this confidence interval is lambda_n +/- 1.96 * se_boot#
    # Let's first compute the MLE#
    lambda_n <- compute_xbar(one_sample)#
#
    # Now we need a bootstrap estimate of it's standard error. #
    # To do this we repeatedly generate bootstrap samples and #
    # compute the MLE each time. Let's write a function that does #
    # this and returns a vector of bootstrapped MLEs#
    .bootstrap_lambda <- function(one_sample){#
        # get a bootstrap sample#
        boot_sample <- get_boot_stample(one_sample)#
        # compute MLE based on boot sample#
        lambda_n_star <- compute_xbar(boot_sample)#
        return(lambda_n_star)#
    }#
#
    # Now we replicate this function B times#
    lambda_n_star_vector <- replicate(B, .bootstrap_lambda(one_sample))#
#
    # The bootstrap standard error estimate is the sd of the#
    # bootstrapped MLEs#
    se_boot <- sd(lambda_n_star_vector)#
#
    # The confidence interval #
    ci <- c(lambda_n - qlogis(0.975) * se_boot,#
            lambda_n + qlogis(0.975) * se_boot)#
#
    # Return the CI#
    return(ci)#
}#
#
# Try it out!#
test_sample <- grab_one_sample(n = 50)#
compute_normal_boot_ci(test_sample)
system.time(compute_normal_boot_ci(test_sample))
system.time(ci_b_100 <- compute_normal_boot_ci(test_sample))
ci_b_100
system.time(ci_b_1000 <- compute_normal_boot_ci(test_sample, B = 1e3))
ci_b_1000
system.time(ci_b_10000 <- compute_normal_boot_ci(test_sample, B = 1e4))
ci_b_10000
compute_pctile_boot_ci <- function(one_sample, B = 1e2){#
    # The form of this confidence interval is the 2.5th and 97.5th#
    # percentiles of the bootstrap distribution of lambda_n#
#
    # We'll use the same function to get a bootstrap estimate of lambda#
    .bootstrap_lambda <- function(one_sample){#
        # get a bootstrap sample#
        boot_sample <- get_boot_stample(one_sample)#
        # compute MLE based on boot sample#
        lambda_n_star <- compute_xbar(boot_sample)#
        return(lambda_n_star)#
    }#
#
    # Now we replicate this function B times#
    lambda_n_star_vector <- replicate(B, .bootstrap_lambda(one_sample))#
#
    # The confidence interval #
    ci <- as.numeric(quantile(lambda_n_star_vector, p = c(0.025, 0.975)))#
#
    # Return the CI#
    return(ci)#
}
# Now let's write a function that will compute a bootstrap#
# percentile interval based on B resamples using Method 3#
# discussed in class (percentile bootstrap interval)#
compute_pctile_boot_ci <- function(one_sample, B = 1e2){#
    # The form of this confidence interval is the 2.5th and 97.5th#
    # percentiles of the bootstrap distribution of lambda_n#
#
    # We'll use the same function to get a bootstrap estimate of lambda#
    .bootstrap_lambda <- function(one_sample){#
        # get a bootstrap sample#
        boot_sample <- get_boot_stample(one_sample)#
        # compute MLE based on boot sample#
        lambda_n_star <- compute_xbar(boot_sample)#
        return(lambda_n_star)#
    }#
#
    # Now we replicate this function B times#
    lambda_n_star_vector <- replicate(B, .bootstrap_lambda(one_sample))#
#
    # The confidence interval #
    ci <- as.numeric(quantile(lambda_n_star_vector, p = c(0.025, 0.975)))#
#
    # Return the CI#
    return(ci)#
}
test_sample <- grab_one_sample(n = 50)
# At B = 100 (default)
system.time(ci_b_100_pctile <- compute_pctile_boot_ci(test_sample))
ci_b_100_pctile
# At B = 10000
system.time(ci_b_10000_pctile <- compute_pctile_boot_ci(test_sample, B = 1e4))
ci_b_10000_pctile
# Let's study the bootstrap distribution of lambda_n relative #
# to the true sampling and to the asymptotic distribution.#
test_sample <- grab_one_sample(n = 50)#
#
# Get the true sampling distribution#
N_experiments <- 1e5#
lots_of_lambdas <- replicate(N_experiments, do_one_experiment(n = 50))#
#
# Now we write a function to generate bootstrap samples based on #
# one_sample and compute the MLE each time. Let's write a function that #
# does this and returns a vector of bootstrapped MLEs#
bootstrap_lambda <- function(one_sample){#
    # get a bootstrap sample#
    boot_sample <- get_boot_stample(one_sample)#
    # compute MLE based on boot sample#
    lambda_n_star <- compute_xbar(boot_sample)#
    return(lambda_n_star)#
}#
#
# Get the bootstrap distribution#
lots_of_lambda_stars <- replicate(1e5, bootstrap_lambda(one_sample))
hist(lots_of_lambda_stars, freq = FALSE, xlab = expression(bar(X)[n]),#
     main = expression("Bootstrap vs. true distribution of "*bar(X)[n]), #
     ylim = c(0,1))
hist(lots_of_lambda_stars, freq = FALSE, xlab = expression(bar(X)[n]),#
     main = expression("Bootstrap vs. true distribution of "*bar(X)[n]), #
     ylim = c(0,2))
hist(lots_of_lambda_stars, freq = FALSE, xlab = expression(bar(X)[n]),
main = expression("Bootstrap vs. true distribution of "*bar(X)[n]),
ylim = c(0,4))
hist(lots_of_lambda_stars, freq = FALSE, xlab = expression(bar(X)[n]),
main = expression("Bootstrap vs. true distribution of "*bar(X)[n]),
ylim = c(0,6))
# Let's study the bootstrap distribution of lambda_n relative #
# to the true sampling and to the asymptotic distribution.#
test_sample <- grab_one_sample(n = 50)#
#
# Get the true sampling distribution#
N_experiments <- 1e5#
lots_of_lambdas <- replicate(N_experiments, do_one_experiment(n = 50))#
#
# Now we write a function to generate bootstrap samples based on #
# one_sample and compute the MLE each time. Let's write a function that #
# does this and returns a vector of bootstrapped MLEs#
bootstrap_lambda <- function(one_sample){#
    # get a bootstrap sample#
    boot_sample <- get_boot_stample(one_sample)#
    # compute MLE based on boot sample#
    lambda_n_star <- compute_xbar(boot_sample)#
    return(lambda_n_star)#
}#
#
# Get the bootstrap distribution#
lots_of_lambda_stars <- replicate(1e5, bootstrap_lambda(test_sample))#
#
# Let's plot the two distributions#
hist(lots_of_lambda_stars, freq = FALSE, xlab = expression(bar(X)[n]),#
     main = expression("Bootstrap vs. true distribution of "*bar(X)[n]), #
     ylim = c(0,6))
# Let's study the bootstrap distribution of lambda_n relative #
# to the true sampling and to the asymptotic distribution.#
test_sample <- grab_one_sample(n = 50)#
#
# Get the true sampling distribution#
N_experiments <- 1e5#
lots_of_lambdas <- replicate(N_experiments, do_one_experiment(n = 50))#
#
# Now we write a function to generate bootstrap samples based on #
# one_sample and compute the MLE each time. Let's write a function that #
# does this and returns a vector of bootstrapped MLEs#
bootstrap_lambda <- function(one_sample){#
    # get a bootstrap sample#
    boot_sample <- get_boot_stample(one_sample)#
    # compute MLE based on boot sample#
    lambda_n_star <- compute_xbar(boot_sample)#
    return(lambda_n_star)#
}#
#
# Get the bootstrap distribution#
lots_of_lambda_stars <- replicate(1e5, bootstrap_lambda(test_sample))#
#
# Let's plot the two distributions#
hist(lots_of_lambda_stars, freq = FALSE, xlab = expression(bar(X)[n]),#
     main = expression("Bootstrap vs. true distribution of "*bar(X)[n]), #
     ylim = c(0,2))
lines(density(lots_of_lambdas, kernel = "epanechnikov"), col = 2, lwd = 2)
# Let's plot the two distributions#
hist(lots_of_lambda_stars, freq = FALSE, xlab = expression(bar(X)[n]),#
     main = expression("Bootstrap vs. true distribution of "*bar(X)[n]), #
     ylim = c(0,2))#
# the "smoothed" bootstrap distribution#
lines(density(lots_of_lambda_stars, kernel = "epanechnikov"), col = 2, lwd = 2)#
#
# the true sampling distribution#
lines(density(lots_of_lambdas, kernel = "epanechnikov"), col = 4, lwd = 2)
legend(x = "topleft", bty = "n", legend = c("Bootstrap dist.", "True dist."),#
       col = c(2, 4), lwd = 2)
